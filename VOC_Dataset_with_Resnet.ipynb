{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIyYEsWfR677nx7g+/TmC7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TSHP/DL4SC_Neural_ODEs/blob/main/VOC_Dataset_with_Resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7ELv7NGmmRT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import numpy\n",
        "from torch.nn import ReLU\n",
        "from torch import optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.functional import one_hot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import VOCSegmentation\n",
        "\n",
        "# Set the root directory where you want to download the dataset\n",
        "root = \"/path/to/save/voc_dataset\"\n",
        "\n",
        "# Define the year and split (train or val) you want to download\n",
        "year = \"2012\"\n",
        "split = \"train\"\n",
        "\n",
        "# Download the VOC dataset\n",
        "voc_dataset = VOCSegmentation(root=root, year=year, image_set=split, download=True)"
      ],
      "metadata": {
        "id": "5h4KlybpmzcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToPILImage\n",
        "\n",
        "# Iterate over the first 5 samples in the dataset\n",
        "for i in range(5):\n",
        "    # Get the image and mask from the dataset\n",
        "    image, mask = voc_dataset[i]\n",
        "\n",
        "    # Convert the image tensor to a PIL image and then to a NumPy array\n",
        "    image_np = np.array(image)\n",
        "\n",
        "    # Convert the mask tensor to a NumPy array\n",
        "    mask_np = np.array(mask)\n",
        "\n",
        "    # Display the image and mask\n",
        "    plt.subplot(5, 2, i * 2 + 1)\n",
        "    plt.imshow(image_np)\n",
        "    plt.title(\"Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(5, 2, i * 2 + 2)\n",
        "    plt.imshow(mask_np, cmap='jet', vmin=0, vmax=21)  # Assuming 21 classes in the segmentation mask\n",
        "    plt.title(\"Segmentation Mask\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1Lgv8fMmmzfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class CustomVOCDataset(Dataset):\n",
        "    def __init__(self, voc_dataset, transform_image=None, transform_mask=None):\n",
        "        self.transform_image = transform_image\n",
        "        self.transform_mask = transform_mask\n",
        "        self.voc_dataset = voc_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.voc_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, mask = self.voc_dataset[idx]\n",
        "\n",
        "        if self.transform_image is not None:\n",
        "            image = self.transform_image(image)\n",
        "        if self.transform_mask is not None:\n",
        "            mask = self.transform_mask(mask)\n",
        "        return image, mask\n",
        "\n",
        "# Set the root directory where the VOC dataset is located\n",
        "data_dir = \"/path/to/voc_dataset\"\n",
        "\n",
        "# Set the year and split you want to use (e.g., \"2012\" and \"train\")\n",
        "year = \"2012\"\n",
        "split = \"train\"\n",
        "\n",
        "out_size = 32\n",
        "# Define your transformations\n",
        "transform_image = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_mask = transforms.Compose([\n",
        "    transforms.Resize((out_size, out_size)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create an instance of your custom dataset with the specified transformations\n",
        "dataset = CustomVOCDataset(voc_dataset, transform_image=transform_image,\n",
        "                           transform_mask=transform_mask)\n",
        "\n",
        "# Create a data loader with the desired batch size and number of workers\n",
        "VOC_data_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "NID75z4zmzhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# Get the first batch from the data loader\n",
        "batch = next(iter(VOC_data_loader))\n",
        "\n",
        "# Extract images and masks from the batch\n",
        "images, masks = batch\n",
        "\n",
        "# Create a grid of images for visualization\n",
        "grid_images = make_grid(images)\n",
        "grid_masks = make_grid(masks)\n",
        "\n",
        "# Convert tensors to numpy arrays\n",
        "grid_images_np = grid_images.numpy().transpose(1, 2, 0)\n",
        "grid_masks_np = grid_masks.numpy().transpose(1, 2, 0)\n",
        "\n",
        "# Display the grid of images and masks\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(grid_images_np)\n",
        "plt.title(\"Images\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(grid_masks_np[:, :, 0])\n",
        "plt.title(\"Masks\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hM51CDRqmzkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(images.shape)\n",
        "print(masks.shape)"
      ],
      "metadata": {
        "id": "64V2-8wdmzmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Define the ResNet model\n",
        "class ResNet6_images(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(ResNet6_images, self).__init__()\n",
        "        w = 32\n",
        "        self.downsample = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=w, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(w),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=w, out_channels=w, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(w),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=w, out_channels=w, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(w),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=w, out_channels=w, kernel_size=3, stride=2, padding=1),\n",
        "        )\n",
        "        self.residual_blocks = nn.Sequential(\n",
        "            ResidualBlock(w, w),\n",
        "            ResidualBlock(w, w),\n",
        "            ResidualBlock(w, w),\n",
        "            ResidualBlock(w, w),\n",
        "            ResidualBlock(w, w),\n",
        "            ResidualBlock(w, w)\n",
        "        )\n",
        "        self.final = nn.Sequential(nn.Conv2d(in_channels=w, out_channels=1, kernel_size=1, stride=1),\n",
        "                      nn.BatchNorm2d(1),\n",
        "                      nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.downsample(x)\n",
        "        out = self.residual_blocks(out)\n",
        "        out = self.final(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "o5X4-ENvmzo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_loss(predicted, target):\n",
        "    smooth = 1e-7  # A small constant to avoid division by zero\n",
        "    intersection = torch.sum(predicted * target)\n",
        "    union = torch.sum(predicted) + torch.sum(target)\n",
        "    dice = (2.0 * intersection + smooth) / (union + smooth)\n",
        "    loss = 1.0 - dice\n",
        "    return loss"
      ],
      "metadata": {
        "id": "OgBsiGIDmzrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the ResNet model\n",
        "model_ResNet = ResNet6_images(in_channels=3)\n",
        "\n",
        "# Define the training parameters\n",
        "learning_rate = 0.001\n",
        "num_epochs = 1\n",
        "print_interval = 3\n",
        "\n",
        "optimizer = optim.Adam(model_ResNet.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    eval_counter = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(VOC_data_loader, 1):\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model_ResNet(images)\n",
        "        loss = dice_loss(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        eval_counter += 1\n",
        "\n",
        "        # Print the running loss every 'print_interval' evaluations\n",
        "        if eval_counter % print_interval == 0:\n",
        "            average_loss = running_loss / print_interval\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Evaluation [{i}/{len(VOC_data_loader)}], Loss: {average_loss}\")\n",
        "            running_loss = 0.0\n",
        "            eval_counter = 0\n",
        "\n",
        "    # Print the remaining running loss at the end of the epoch\n",
        "    if eval_counter > 0:\n",
        "        average_loss = running_loss / eval_counter\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Evaluation [{i}/{len(VOC_data_loader)}], Loss: {average_loss}\")"
      ],
      "metadata": {
        "id": "dzD5t1R4mztU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Di0Bi3xVmzva"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}